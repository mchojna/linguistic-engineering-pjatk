{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# <p style=\"color: #00BFFF;\">**wiczenia 10** </p>  \n",
    "# <p style=\"color: #00BFFF\">*Pre-trained word embeddings* </p>\n",
    "\n",
    "Embeddingi to wektory reprezentujce sowa w przestrzeni wielowymiarowej. \n",
    "\n",
    "S u偶ywane do przeksztacania tekstu w form zrozumia dla modeli maszynowego uczenia. \n",
    "\n",
    "Jest du偶o gotowych przetrenowanych ju偶 zestaw贸w word embeddings, kt贸re mo偶na u偶y.\n",
    "\n",
    "Popularne modele embedding贸w to:\n",
    "\n",
    "\n",
    " * **googlowskie  Word2vec**  u偶ywa prediction model dla przewidzenia nastpnego sowa w zdaniu.\n",
    " \n",
    "    Uczy si na podstawie kontekstu s贸w.\n",
    "\n",
    "* **stanfordzkie GloVe**  u偶ywa count-based approach ( tak jak w macierzy word-word) i redukuje liczb wymiar贸w.\n",
    "\n",
    "    Uczy si na podstawie globalnych statystyk s贸w.\n",
    "\n",
    "* **BERT** (Bidirectional Encoder Representations from Transformers) uczcy si na podstawie kontekstu caych zda.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spos贸b przedstawienia znaczenia sowa w postaci wektorowej (tablica liczb), daje mo偶liwo wykonania r贸偶nych \"dziaa\" na sowach. \n",
    "\n",
    "Klasycznym przykadem jest: \n",
    "\n",
    "Pary偶-Francja+Wochy=Rzym\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color: orange;\">Zadanie 1 </p>  \n",
    "Sprawd偶my czy NLP dziaa.\n",
    "\n",
    "*( tak naprawde sprawdzimy tylko czy dziaa \"matematyka\" na sowach, ale mo偶liwo traktowania s贸w jak wektor贸w jest punktem wyjcia do wszystkich ( albo prawie wszystkich) zada przetwarzania jzyka naturalnego)*\n",
    "\n",
    "Spr贸bujemy wymodelowa dziaanie na wektorach i sprawdzi do jakiego wektora wynik jest najbardziej podobny w sensie podobiestwa znaczenia sowa. ( podobiestwo wektor贸w wystarczy, sprawdzilimy ju偶 wczesniej 偶e to dziaa nawet dla najprostszej miary cosinusowej)\n",
    "\n",
    "\n",
    "U偶yjemy Glove, korpus 6B.50d, z poni偶szego linka (porednio, cigamy zipa i zostawiamy tylko jeden plik, wa偶ne w jakim katalogu 偶eby dobrze si zlinkowao).\n",
    "Zostawiamy najmniejszy plik, 偶ebymy zd偶yli policzy. (mo偶na znale藕 link do pojedynczego zbiory glove.6B.50d.txt, wtedy nie ma potrzeby kasowania)\n",
    "\n",
    "\n",
    "To jest 50 wymiarowe embeddings, co oznacza 偶e ka偶dy wektor ma 50 wsp贸rzdnych.\n",
    "\n",
    "Stanford Glove website: \n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/.\n",
    "\n",
    "\n",
    "\n",
    "glove.6B.50d.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "path = r\"models/glove.6B.50d.txt\"\n",
    "words = pd.read_table(path, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otrzymamy 50 wymiarowe wektory z 400k s贸w.\n",
    "\n",
    "Mo偶emy obejrze jeden z nich, na przykad wektor\n",
    "\n",
    "*Paris*\n",
    "\n",
    "**Wywietl wektor Paris** i wyjanij znaczenie wsp贸rzdnych tego wektora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "9",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "11",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "12",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "13",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "14",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "16",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "17",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "18",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "19",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "20",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "21",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "22",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "23",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "24",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "25",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "26",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "27",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "28",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "29",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "30",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "31",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "32",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "33",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "34",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "35",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "36",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "37",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "38",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "39",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "40",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "41",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "42",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "43",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "44",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "45",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "46",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "47",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "48",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "49",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "50",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "312e30d4-0082-4829-847b-4b57db95201e",
       "rows": [
        [
         "paris",
         "0.76989",
         "1.181",
         "-1.1299",
         "-0.74725",
         "-0.5969",
         "-1.0518",
         "-0.46552",
         "0.27009",
         "-0.99243",
         "-0.04864",
         "0.28642",
         "-0.75261",
         "-1.0566",
         "-0.19205",
         "0.572",
         "-0.24391",
         "-0.36054",
         "-0.70876",
         "-0.91951",
         "-0.27024",
         "1.5131",
         "1.0313",
         "-0.55713",
         "0.52952",
         "-0.71494",
         "-1.0949",
         "-0.60565",
         "0.31329",
         "-0.44488",
         "0.55915",
         "2.1429",
         "0.43389",
         "-0.5529",
         "-0.24261",
         "-0.43679",
         "-0.96014",
         "0.25828",
         "0.79385",
         "0.37132",
         "0.49623",
         "0.84359",
         "-0.25875",
         "1.5616",
         "-1.1199",
         "0.091676",
         "0.076675",
         "-0.45084",
         "-0.86104",
         "0.97599",
         "-0.35615"
        ]
       ],
       "shape": {
        "columns": 50,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>paris</th>\n",
       "      <td>0.76989</td>\n",
       "      <td>1.181</td>\n",
       "      <td>-1.1299</td>\n",
       "      <td>-0.74725</td>\n",
       "      <td>-0.5969</td>\n",
       "      <td>-1.0518</td>\n",
       "      <td>-0.46552</td>\n",
       "      <td>0.27009</td>\n",
       "      <td>-0.99243</td>\n",
       "      <td>-0.04864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.84359</td>\n",
       "      <td>-0.25875</td>\n",
       "      <td>1.5616</td>\n",
       "      <td>-1.1199</td>\n",
       "      <td>0.091676</td>\n",
       "      <td>0.076675</td>\n",
       "      <td>-0.45084</td>\n",
       "      <td>-0.86104</td>\n",
       "      <td>0.97599</td>\n",
       "      <td>-0.35615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1      2       3        4       5       6        7        8   \\\n",
       "0                                                                          \n",
       "paris  0.76989  1.181 -1.1299 -0.74725 -0.5969 -1.0518 -0.46552  0.27009   \n",
       "\n",
       "            9        10  ...       41       42      43      44        45  \\\n",
       "0                        ...                                               \n",
       "paris -0.99243 -0.04864  ...  0.84359 -0.25875  1.5616 -1.1199  0.091676   \n",
       "\n",
       "             46       47       48       49       50  \n",
       "0                                                    \n",
       "paris  0.076675 -0.45084 -0.86104  0.97599 -0.35615  \n",
       "\n",
       "[1 rows x 50 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kod \n",
    "words[words.index == \"paris\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz znajd藕my embedding dla s贸w, kt贸re si w tym zbiorze znajduj:\n",
    "\n",
    "*France, Italy, Paris, Rome*\n",
    "\n",
    "\n",
    "mo偶na u偶y np. *word loc*, ale...\n",
    "\n",
    " nie trzeba; mo偶na stworzy macierz dowolna metod\n",
    "\n",
    "Wywietl macierz kt贸ra powstanie, oraz macierz po normalizacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#miejsce na kod macierzy coocurence, (znormalizowanej r贸wnie偶)\n",
    "import numpy as np\n",
    "\n",
    "selected_words = [\"france\", \"italy\", \"paris\", \"rome\"]\n",
    "\n",
    "embeddings = words.loc[selected_words]\n",
    "\n",
    "coocurence_matrix = np.dot(embeddings, embeddings.T)\n",
    "coocurence_df = pd.DataFrame(coocurence_matrix, index=selected_words, columns=selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "france",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "italy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "paris",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rome",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "96ccfb03-bd1c-40c2-9de1-3af3a8f638a1",
       "rows": [
        [
         "france",
         "35.617476359883206",
         "26.39846926218464",
         "26.571788040327995",
         "17.723144661925105"
        ],
        [
         "italy",
         "26.39846926218464",
         "32.253093384613365",
         "18.623831698975998",
         "20.89440262776776"
        ],
        [
         "paris",
         "26.571788040327995",
         "18.623831698975998",
         "30.77887021710101",
         "19.29898620805099"
        ],
        [
         "rome",
         "17.723144661925105",
         "20.89440262776776",
         "19.29898620805099",
         "24.008865411037647"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>france</th>\n",
       "      <th>italy</th>\n",
       "      <th>paris</th>\n",
       "      <th>rome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>france</th>\n",
       "      <td>35.617476</td>\n",
       "      <td>26.398469</td>\n",
       "      <td>26.571788</td>\n",
       "      <td>17.723145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italy</th>\n",
       "      <td>26.398469</td>\n",
       "      <td>32.253093</td>\n",
       "      <td>18.623832</td>\n",
       "      <td>20.894403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paris</th>\n",
       "      <td>26.571788</td>\n",
       "      <td>18.623832</td>\n",
       "      <td>30.778870</td>\n",
       "      <td>19.298986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rome</th>\n",
       "      <td>17.723145</td>\n",
       "      <td>20.894403</td>\n",
       "      <td>19.298986</td>\n",
       "      <td>24.008865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           france      italy      paris       rome\n",
       "france  35.617476  26.398469  26.571788  17.723145\n",
       "italy   26.398469  32.253093  18.623832  20.894403\n",
       "paris   26.571788  18.623832  30.778870  19.298986\n",
       "rome    17.723145  20.894403  19.298986  24.008865"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coocurence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = coocurence_matrix.sum(axis=1, keepdims=True)\n",
    "normalized_matrix = coocurence_matrix / row_sums\n",
    "normalized_df = pd.DataFrame(normalized_matrix, index=selected_words, columns=selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "france",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "italy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "paris",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rome",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0c37a6c4-f857-4fb2-bc93-42773a72a6fc",
       "rows": [
        [
         "france",
         "0.3350313431822614",
         "0.24831390426153044",
         "0.24994420570269257",
         "0.16671054685351563"
        ],
        [
         "italy",
         "0.2689062224433389",
         "0.3285439552585207",
         "0.18971040251815333",
         "0.21283941977998708"
        ],
        [
         "paris",
         "0.2789001630890553",
         "0.1954776129594404",
         "0.32305812127577005",
         "0.20256410267573435"
        ],
        [
         "rome",
         "0.2163327234043578",
         "0.2550418173859891",
         "0.23556780272182906",
         "0.29305765648782406"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>france</th>\n",
       "      <th>italy</th>\n",
       "      <th>paris</th>\n",
       "      <th>rome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>france</th>\n",
       "      <td>0.335031</td>\n",
       "      <td>0.248314</td>\n",
       "      <td>0.249944</td>\n",
       "      <td>0.166711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italy</th>\n",
       "      <td>0.268906</td>\n",
       "      <td>0.328544</td>\n",
       "      <td>0.189710</td>\n",
       "      <td>0.212839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paris</th>\n",
       "      <td>0.278900</td>\n",
       "      <td>0.195478</td>\n",
       "      <td>0.323058</td>\n",
       "      <td>0.202564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rome</th>\n",
       "      <td>0.216333</td>\n",
       "      <td>0.255042</td>\n",
       "      <td>0.235568</td>\n",
       "      <td>0.293058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          france     italy     paris      rome\n",
       "france  0.335031  0.248314  0.249944  0.166711\n",
       "italy   0.268906  0.328544  0.189710  0.212839\n",
       "paris   0.278900  0.195478  0.323058  0.202564\n",
       "rome    0.216333  0.255042  0.235568  0.293058"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No i sprawd偶my czy matematyka na sowach zadziaa\n",
    "\n",
    "czyli\n",
    "\n",
    "*Pary偶-Francja+Wochy=Rzym*\n",
    "\n",
    "Stw贸rz wektor Paris-France+Italy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# miejsce na kod\n",
    "left = words.loc[\"paris\"].values - words.loc[\"france\"] + words.loc[\"italy\"]\n",
    "right = words.loc[\"rome\"]\n",
    "(left == right).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zbadaj podobiestwo pomidzy stworzonym wektorem a innymi sowami wywietlajc 10 najbardziej podobnych do niego s贸w \n",
    "\n",
    "( np.  u偶ywajc find_closest_word, ale mo偶na te偶 uzy innej metody liczenia podobiestwa, szukaj metod similarity, \n",
    "\n",
    "wybierajc metode pamitaj, 偶e poruszamy si wywa偶ajc korzyci midzy jakoci wyniku a czasem oblicze)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.87599809,  9.74306152, 10.58793184, ..., -8.42813627,\n",
       "       -3.25354317, -3.25354317])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(words, right.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.dot(words, right.T)\n",
    "b = pd.DataFrame(a, index=words.index, columns=[\"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "distance",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f63da03a-62eb-4d94-8ea0-3664dbbb0a33",
       "rows": [
        [
         "rome",
         "24.008865411037647"
        ],
        [
         "italy",
         "20.89440262776776"
        ],
        [
         "cathedral",
         "20.577377084179993"
        ],
        [
         "milan",
         "20.175302951533002"
        ],
        [
         "sicherle",
         "20.092325977033994"
        ],
        [
         "pope",
         "20.001254254248998"
        ],
        [
         "madrid",
         "19.379516042379997"
        ],
        [
         "paris",
         "19.29898620805099"
        ],
        [
         "roman",
         "19.273272572593996"
        ],
        [
         "palace",
         "18.8550908088694"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rome</th>\n",
       "      <td>24.008865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italy</th>\n",
       "      <td>20.894403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cathedral</th>\n",
       "      <td>20.577377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milan</th>\n",
       "      <td>20.175303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sicherle</th>\n",
       "      <td>20.092326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pope</th>\n",
       "      <td>20.001254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>madrid</th>\n",
       "      <td>19.379516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paris</th>\n",
       "      <td>19.298986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roman</th>\n",
       "      <td>19.273273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>palace</th>\n",
       "      <td>18.855091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            distance\n",
       "0                   \n",
       "rome       24.008865\n",
       "italy      20.894403\n",
       "cathedral  20.577377\n",
       "milan      20.175303\n",
       "sicherle   20.092326\n",
       "pope       20.001254\n",
       "madrid     19.379516\n",
       "paris      19.298986\n",
       "roman      19.273273\n",
       "palace     18.855091"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kod \n",
    "def find_closest_words(words, embedding, top_n=10):\n",
    "    distances = np.dot(words, embedding.T)\n",
    "    coocurence_df = pd.DataFrame(distances, index=words.index, columns=[\"distance\"]).sort_values(by=[\"distance\"], ascending=False)\n",
    "    return coocurence_df.iloc[0:top_n, :]\n",
    "\n",
    "find_closest_words(words, right, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czy najbardziej podobny do tego  wektora Paris-France+Italy jest wektor Rome ?\n",
    "\n",
    "Czy to oznacza 偶e tak relacje mo偶na powt贸rzy dla innych zestaw贸w s贸w?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color: yellow;\">Zadanie 2 </p> \n",
    "\n",
    "\n",
    " Powt贸rz dziaania dla poni偶szych zestaw贸w :\n",
    "\n",
    "**water-wet+fire= flames**\n",
    "\n",
    "\n",
    "**winter-cold+summer=warm**\n",
    "\n",
    "\n",
    "oraz klasycznego:\n",
    "\n",
    "**king-men+woman=queen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kod zadania 2\n",
    "def compare_vectors(index_1, index_2, index_3, index_4, words, top_n=10):\n",
    "    left = words.loc[index_1] - words.loc[index_2] + words.loc[index_3]\n",
    "    right = words.loc[index_4]\n",
    "    \n",
    "    print(f\"Comparison: {(left==right).sum()}\")\n",
    "    \n",
    "    coocurence_df = find_closest_words(words, right, 10)\n",
    "    print(coocurence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: 0\n",
      "            distance\n",
      "0                   \n",
      "flames     28.240650\n",
      "nahn       22.579730\n",
      "debris     20.848968\n",
      "billowed   20.623328\n",
      "smoke      20.148238\n",
      "tear       19.056865\n",
      "billowing  18.841191\n",
      "blaze      18.731457\n",
      "avalanche  18.456331\n",
      "debehogne  18.378496\n"
     ]
    }
   ],
   "source": [
    "compare_vectors(\"water\", \"wet\", \"fire\", \"flames\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: 0\n",
      "                 distance\n",
      "0                        \n",
      "warm            29.908686\n",
      "1-800-535-4425  28.739116\n",
      "moist           24.262903\n",
      "dry             23.730344\n",
      "temperatures    22.507354\n",
      "drizzle         22.437515\n",
      "warmer          22.102078\n",
      "humid           22.089390\n",
      "rain            21.985990\n",
      "cold            21.807384\n"
     ]
    }
   ],
   "source": [
    "compare_vectors(\"winter\", \"cold\", \"summer\", \"warm\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: 0\n",
      "           distance\n",
      "0                  \n",
      "queen     26.964749\n",
      "princess  25.256698\n",
      "king      21.877506\n",
      "throne    21.784407\n",
      "royal     21.776677\n",
      "daughter  21.687536\n",
      "her       21.545704\n",
      "empress   20.825105\n",
      "prince    20.645073\n",
      "mother    20.501926\n"
     ]
    }
   ],
   "source": [
    "compare_vectors(\"king\", \"men\", \"women\", \"queen\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opr贸cz GloVe, istnieje kilka innych pretrenowanych embedding贸w, kt贸re mog by u偶yte do klasycznego zadania analogii, takiego jak \n",
    "\n",
    "\"kr贸l - m偶czyzna + kobieta = kr贸lowa\" (Word2Vec i FasTex powinny dziaa r贸wnie偶 po polsku). \n",
    "\n",
    "## <p style=\"color: #6495ED;\">Zadanie 3 </p> \n",
    "Pobierz inne pretrenowane modele (np. Word2Vec, FastText, BERT)\n",
    "<p style=\"color: red;\">[dla chetnych] </p>  ELMo (Embeddings from Language Models) \n",
    "\n",
    "i dla ka偶dego z nich powt贸rz zestaw angielskich analogii z poprzednich zada.\n",
    "\n",
    "Nastpnie spr贸buj doda polskie:\n",
    "\n",
    "* **Warszawa - Polska + Niemcy = Berlin** \n",
    "* **Kr贸l - m偶czyzna + kobieta = kr贸lowa** \n",
    "* **Pary偶 - Francja + Wochy = Rzym**\n",
    "* **Siostra - kobieta + m偶czyzna = brat** \n",
    "\n",
    "Por贸wnaj jako dziaania r贸znych modeli, r贸wnie偶 dla polskich zestaw贸w. \n",
    "Mo偶esz spr贸bowa poprawi wyniki znajdujc polskie modele.\n",
    "\n",
    "Biblioteki:\n",
    "\n",
    "* pip install gensim (word2Vec, FastTex)\n",
    "* pip install transformers torch (BERT)\n",
    "* <p style=\"color: red;\">[dla chetnych] </p>  pip install allennlp (ElMo)\n",
    "\n",
    "\n",
    "Modele:\n",
    "* word2vec-google-news-300\n",
    "* fasttext-wiki-news-subwords-300\n",
    "* bert-base-uncased\n",
    "* <p style=\"color: red;\">[dla chetnych] </p>   Konfiguracja ELMo:\n",
    "\n",
    "    options_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096x512_2048cnn_2xhighway/elmo_2x4096x512_2048cnn_2xhighway_options.json\"\n",
    "\n",
    "    weight_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096x512_2048cnn_2xhighway/elmo_2x4096x512_2048cnn_2xhighway_weights.hdf5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kod zadania 3\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "fasttext_model = KeyedVectors.load(r'models/fasttext-wiki-news-subwords-300.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity_f(model, words):\n",
    "    left = model[words[0]] - model[words[1]] + model[words[2]]\n",
    "    right = model[words[3]]\n",
    "\n",
    "    print(f\"Comparison: {(left == right).sum()}\")\n",
    "    print(f\"Similar: {model.most_similar(left)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: 0\n",
      "Similar: [('king', 0.8453638553619385), ('queen', 0.71834397315979), ('king-', 0.678547203540802), ('monarch', 0.6708823442459106), ('child-king', 0.6690906286239624), ('boy-king', 0.6576582789421082), ('king-making', 0.6454074382781982), ('kingship', 0.6315165758132935), ('princess', 0.6250572800636292), ('prince', 0.6195020079612732)]\n",
      "Comparison: 0\n",
      "Similar: [('Italy', 0.8525390028953552), ('Rome', 0.8008622527122498), ('Milan', 0.792957067489624), ('Bologna', 0.784528374671936), ('Naples', 0.7842305302619934), ('Italy-', 0.7604862451553345), ('Paris', 0.7551395297050476), ('Venice', 0.7510690093040466), ('Milano', 0.7439693808555603), ('Parioli', 0.7396515011787415)]\n",
      "Comparison: 0\n",
      "Similar: [('Berlin', 0.7403421401977539), ('Germany', 0.7287525534629822), ('Warsaw', 0.7193452715873718), ('Munich', 0.7039551734924316), ('Hamburg', 0.6924000978469849), ('Stuttgart', 0.6905930042266846), ('Frankfurt', 0.6903912425041199), ('Bonn', 0.6753316521644592), ('Cologne', 0.6714510321617126), ('Dresden', 0.6614290475845337)]\n",
      "Comparison: 0\n",
      "Similar: [('sister', 0.846070408821106), ('brother', 0.8282320499420166), ('brothers', 0.727699339389801), ('sisters', 0.7184571027755737), ('brother-in-law', 0.7170220017433167), ('cousin', 0.7140408158302307), ('sister-in-law', 0.6763167381286621), ('stepbrother', 0.6760019063949585), ('uncle', 0.6749981641769409), ('cousins', 0.6727116703987122)]\n"
     ]
    }
   ],
   "source": [
    "check_similarity_f(fasttext_model, [\"king\", \"men\", \"women\", \"queen\"])\n",
    "check_similarity_f(fasttext_model, [\"Paris\", \"France\", \"Italy\", \"Rome\"])\n",
    "check_similarity_f(fasttext_model, [\"Warsaw\", \"Poland\", \"Germany\", \"Berlin\"])\n",
    "check_similarity_f(fasttext_model, [\"sister\", \"women\", \"men\", \"brother\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(r\"models/bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(r\"models/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity_b(model, tokenizer, words):\n",
    "    word_embeddings_matrix = model.embeddings.word_embeddings.weight\n",
    "    \n",
    "    ids_words = tokenizer(words, add_special_tokens=False)['input_ids']\n",
    "    ids_words = [id_list[0] for id_list in ids_words]\n",
    "    \n",
    "    embedding_1 = word_embeddings_matrix[ids_words[0]] \n",
    "    embedding_2 = word_embeddings_matrix[ids_words[1]] \n",
    "    embedding_3 = word_embeddings_matrix[ids_words[2]] \n",
    "    embedding_4 = word_embeddings_matrix[ids_words[3]] \n",
    "    \n",
    "    result_vector = embedding_1 - embedding_2 + embedding_3\n",
    "    print(f\"Comparison: {(result_vector == embedding_4).sum()}\")\n",
    "    \n",
    "    result_vector_normalized = F.normalize(result_vector.unsqueeze(0), p=2, dim=1)\n",
    "    word_embeddings_matrix_normalized = F.normalize(word_embeddings_matrix, p=2, dim=1)\n",
    "    \n",
    "    similarities = torch.matmul(result_vector_normalized, word_embeddings_matrix_normalized.transpose(0, 1))\n",
    "    \n",
    "    similarities = similarities.squeeze(0)\n",
    "    \n",
    "    sorted_indices = torch.argsort(similarities, descending=True)\n",
    "    top_k = 5\n",
    "    top_k_indices = sorted_indices[ : 3 + top_k]\n",
    "    top_k_similarities = similarities[top_k_indices]\n",
    "    top_k_words = tokenizer.convert_ids_to_tokens(sorted_indices)\n",
    "\n",
    "    \n",
    "    print(f\"Similar:\")\n",
    "    for word, sim in zip(top_k_words, top_k_similarities):\n",
    "        print(f\"  {word}: {sim.item():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: 0\n",
      "Similar:\n",
      "  king: 0.7536\n",
      "  queen: 0.6097\n",
      "  women: 0.4743\n",
      "  kings: 0.4737\n",
      "  queens: 0.4304\n",
      "  prince: 0.4302\n",
      "  princess: 0.4298\n",
      "  ruler: 0.4134\n",
      "Comparison: 0\n",
      "Similar:\n",
      "  warsaw: 0.7318\n",
      "  germany: 0.6584\n",
      "  berlin: 0.5645\n",
      "  munich: 0.5633\n",
      "  germans: 0.5404\n",
      "  darmstadt: 0.5385\n",
      "  vienna: 0.5370\n",
      "  stuttgart: 0.5361\n",
      "Comparison: 0\n",
      "Similar:\n",
      "  paris: 0.7137\n",
      "  italy: 0.6913\n",
      "  italian: 0.5560\n",
      "  milan: 0.5440\n",
      "  parisian: 0.5329\n",
      "  italians: 0.5269\n",
      "  naples: 0.5162\n",
      "  rome: 0.5144\n",
      "Comparison: 0\n",
      "Similar:\n",
      "  sister: 0.7488\n",
      "  brother: 0.6498\n",
      "  daughter: 0.4801\n",
      "  men: 0.4801\n",
      "  son: 0.4672\n",
      "  father: 0.4654\n",
      "  sisters: 0.4611\n",
      "  niece: 0.4503\n"
     ]
    }
   ],
   "source": [
    "check_similarity_b(bert_model, bert_tokenizer, [\"king\", \"men\", \"women\", \"queen\"])\n",
    "check_similarity_b(bert_model, bert_tokenizer, [\"Warsaw\", \"Poland\", \"Germany\", \"Berlin\"])\n",
    "check_similarity_b(bert_model, bert_tokenizer, [\"Paris\", \"France\", \"Italy\", \"Rome\"])\n",
    "check_similarity_b(bert_model, bert_tokenizer, [\"sister\", \"women\", \"men\", \"brother\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_model = KeyedVectors.load(r\"models/word2vec-google-news-300/word2vec-google-news-300.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(model, words):\n",
    "    left = model[words[0]] - model[words[1]] + model[words[2]]\n",
    "    right = model[words[3]]\n",
    "\n",
    "    print(f\"Comparison: {(left == right).sum()}\")\n",
    "    print(f\"Similar: {model.most_similar(left)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: 1\n",
      "Similar: [('king', 0.8527070879936218), ('queen', 0.6743921637535095), ('monarch', 0.6191632151603699), ('kings', 0.5753854513168335), ('crown_prince', 0.562209963798523), ('princess', 0.543317437171936), ('prince', 0.5246986150741577), ('sultan', 0.5236638784408569), ('ruler', 0.5165805220603943), ('monarchy', 0.5113592147827148)]\n",
      "Comparison: 1\n",
      "Similar: [('Milan', 0.7251641750335693), ('Rome', 0.7073156237602234), ('Italy', 0.684763491153717), ('Paris', 0.6693719625473022), ('Palermo_Sicily', 0.6014109253883362), ('Italian', 0.596384584903717), ('Tuscany', 0.5672276020050049), ('Sicily', 0.5654013752937317), ('Bologna', 0.5645887851715088), ('Bologna_Italy', 0.5513802170753479)]\n",
      "Comparison: 1\n",
      "Similar: [('Berlin', 0.7192326188087463), ('Warsaw', 0.7112792134284973), ('Frankfurt', 0.6706617474555969), ('Dusseldorf', 0.6601549386978149), ('Stuttgart', 0.6353572607040405), ('Munich', 0.6288971900939941), ('Germany', 0.6251928806304932), ('Vienna', 0.6169423460960388), ('Hamburg', 0.6073066592216492), ('Cologne', 0.6056288480758667)]\n",
      "Comparison: 0\n",
      "Similar: [('sister', 0.8361483812332153), ('brother', 0.7752998471260071), ('cousin', 0.7571418285369873), ('uncle', 0.666831374168396), ('younger_brother', 0.6621278524398804), ('nephew', 0.6567128300666809), ('son', 0.6531183123588562), ('aunt', 0.6464547514915466), ('father', 0.6464194655418396), ('siblings', 0.6383066177368164)]\n"
     ]
    }
   ],
   "source": [
    "check_similarity(word2vec_model, [\"king\", \"men\", \"women\", \"queen\"])\n",
    "check_similarity(word2vec_model, [\"Paris\", \"France\", \"Italy\", \"Rome\"])\n",
    "check_similarity(word2vec_model, [\"Warsaw\", \"Poland\", \"Germany\", \"Berlin\"])\n",
    "check_similarity(word2vec_model, [\"sister\", \"women\", \"men\", \"brother\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kt贸re embeddingi lepiej odwzorowuj zale偶noci midzy sowami ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color: magenta;\">Zadanie 4 </p> \n",
    "\n",
    "U偶yj pretrenowanych embedding贸w do pokazania wieloznacznoci sowa \"bank\". \n",
    "\n",
    "Zadanie polega na znalezieniu najbli偶szych angielskich s贸w do podanego niejednoznaczmego sowa i ustalenia kontekstu \n",
    "*<p style=\"color:yellow;\">\"bank\"</p>* \n",
    "\n",
    "(\"bank\" jako instytucja finansowa i \"bank\" jako brzeg rzeki)\n",
    "\n",
    "U偶yj modelu Word2Vec. \n",
    "\n",
    "Znajd藕 najbli偶sze (najbardziej podobne ) sowa ( 10 z najwyzszym podobiestwem) i na tej podstawie ustal kontekst w jakim sowo zostao zrozumiane.\n",
    "Mozna spr贸bowac wymusi konkretny kontekst szukajc podobiestw do \"river_bank\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('banks', 0.7440759539604187),\n",
       " ('banking', 0.6901614665985107),\n",
       " ('Bank', 0.6698698401451111),\n",
       " ('lender', 0.634228527545929),\n",
       " ('banker', 0.6092953681945801),\n",
       " ('depositors', 0.6031531691551208),\n",
       " ('mortgage_lender', 0.5797975659370422),\n",
       " ('depositor', 0.5716428160667419),\n",
       " ('BofA', 0.5714625120162964),\n",
       " ('Citibank', 0.5589520931243896)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kod zadania 4\n",
    "\n",
    "word2vec_model.most_similar(\"bank\"[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color: pink;\">Zadanie 5 </p> \n",
    "\n",
    "Powt贸rz zadanie 4 dla : \n",
    " \n",
    "  - *bat* (nietoperz vs kij do baseballa) \n",
    "  - *apple* (owoc vs firma technologiczna) \n",
    "  - *python* (jzyk programowania vs w偶)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apples', 0.720359742641449),\n",
       " ('pear', 0.6450697183609009),\n",
       " ('fruit', 0.6410146355628967),\n",
       " ('berry', 0.6302294731140137),\n",
       " ('pears', 0.6133960485458374),\n",
       " ('strawberry', 0.6058261394500732),\n",
       " ('peach', 0.6025872826576233),\n",
       " ('potato', 0.5960935354232788),\n",
       " ('grape', 0.5935865044593811),\n",
       " ('blueberry', 0.5866668224334717)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kod zadania 5\n",
    "word2vec_model.most_similar(\"apple\"[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bats', 0.7677518725395203),\n",
       " ('batting', 0.6346984505653381),\n",
       " ('Pinch_hitter_Brayan_Pena', 0.6011941432952881),\n",
       " ('batsman', 0.5579798817634583),\n",
       " ('batted', 0.5542199611663818),\n",
       " ('Hawaiian_hoary', 0.5447419881820679),\n",
       " ('Lelands.com_auctioned', 0.5397745370864868),\n",
       " ('yelled_Cheater', 0.5380043983459473),\n",
       " ('wicketkeeper_Andrew_Hodd', 0.5371009111404419),\n",
       " ('lefthanded_batter', 0.53566575050354)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(\"bat\"[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pythons', 0.6688377857208252),\n",
       " ('Burmese_python', 0.6680365204811096),\n",
       " ('snake', 0.6606293320655823),\n",
       " ('crocodile', 0.6591362953186035),\n",
       " ('boa_constrictor', 0.6443519592285156),\n",
       " ('alligator', 0.6421657204627991),\n",
       " ('reptile', 0.6387745141983032),\n",
       " ('albino_python', 0.6158880591392517),\n",
       " ('croc', 0.6083583831787109),\n",
       " ('lizard', 0.6013416647911072)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(\"python\"[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color: #7FFFD4\">Zadanie 6 </p> \n",
    "Powt贸rz instrukcj zadania 4 dla polskich s贸w o r贸偶nych znaczeniach: \n",
    "  - *zamek* (budowla vs mechanizm zamykajcy), \n",
    "  - *pia* (narzdzie vs czasownik w przeszoci)\n",
    "  - *list* (zapis wiadomoci vs nachylenie statku; w kontekcie 偶eglugi, \"list\" oznacza kt nachylenia statku na boki (do portu lub do sterburty) w stanie r贸wnowagi, bez dziaania zewntrznych si\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Zamek', 0.8126124739646912),\n",
       " ('z谩mek', 0.8014318943023682),\n",
       " ('Ogr贸d', 0.7847594618797302),\n",
       " ('zamku', 0.7834646701812744),\n",
       " ('rynek', 0.7772862911224365),\n",
       " ('rzecz', 0.7753502130508423),\n",
       " ('Dziwn贸w', 0.7723685503005981),\n",
       " ('rzeczy', 0.7715327143669128),\n",
       " ('Ksi偶', 0.7680349946022034),\n",
       " ('Podzamcze', 0.7647734880447388)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kod zadania 6\n",
    "fasttext_model.most_similar(\"zamek\"[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pilau', 0.7341576218605042),\n",
       " ('pilav', 0.7166672348976135),\n",
       " ('pilum', 0.6975015997886658),\n",
       " ('palla', 0.6828243136405945),\n",
       " ('Mpila', 0.6811730861663818),\n",
       " ('pilaris', 0.6811076402664185),\n",
       " ('maro', 0.6795235872268677),\n",
       " ('bura', 0.6751182079315186),\n",
       " ('pilate', 0.6681479811668396),\n",
       " ('pilar', 0.6631638407707214)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.most_similar(\"pila\"[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lists', 0.84706711769104),\n",
       " ('listing', 0.7999898195266724),\n",
       " ('sub-list', 0.7774856686592102),\n",
       " ('listed', 0.7744861841201782),\n",
       " ('list-', 0.7545185089111328),\n",
       " ('lists-', 0.7412810325622559),\n",
       " ('non-list', 0.739302933216095),\n",
       " ('list--', 0.7241465449333191),\n",
       " ('list-table', 0.7234554886817932),\n",
       " ('listable', 0.7148085832595825)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.most_similar(\"list\"[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Wnioski\n",
    "* Kt贸ry model dziaa najlepiej?\n",
    "\n",
    "\n",
    "* Kt贸ry model dziaa najszybciej?\n",
    "\n",
    "* Kt贸ry by najatwiejszy w uzyciu?\n",
    "\n",
    "\n",
    "* Kt贸ry zadziaa dla polskiego?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
