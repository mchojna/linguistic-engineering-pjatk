{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤔\n",
    "# <p style=\"color: #00BFFF;\">**Ćwiczenia 10** </p>  \n",
    "# <p style=\"color: #00BFFF\">*Pre-trained word embeddings* </p>\n",
    "\n",
    "Embeddingi to wektory reprezentujące słowa w przestrzeni wielowymiarowej. \n",
    "\n",
    "Są używane do przekształcania tekstu w formę zrozumiałą dla modeli maszynowego uczenia. \n",
    "\n",
    "Jest dużo gotowych przetrenowanych już zestawów word embeddings, które można użyć.\n",
    "\n",
    "Popularne modele embeddingów to:\n",
    "\n",
    "\n",
    " * **googlowskie  Word2vec**  używa prediction model dla przewidzenia następnego słowa w zdaniu.\n",
    " \n",
    "    Uczy się na podstawie kontekstu słów.\n",
    "\n",
    "* **stanfordzkie GloVe**  używa count-based approach ( tak jak w macierzy word-word) i redukuje liczbę wymiarów.\n",
    "\n",
    "    Uczy się na podstawie globalnych statystyk słów.\n",
    "\n",
    "* **BERT** (Bidirectional Encoder Representations from Transformers) uczący się na podstawie kontekstu całych zdań.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sposób przedstawienia znaczenia słowa w postaci wektorowej (tablica liczb), daje możliwość wykonania różnych \"działań\" na słowach. \n",
    "\n",
    "Klasycznym przykładem jest: \n",
    "\n",
    "Paryż-Francja+Włochy=Rzym\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color: orange;\">Zadanie 1 </p>  🐸\n",
    "Sprawdżmy czy NLP działa.\n",
    "\n",
    "*( tak naprawde sprawdzimy tylko czy działa \"matematyka\" na słowach, ale możliwość traktowania słów jak wektorów jest punktem wyjścia do wszystkich ( albo prawie wszystkich) zadań przetwarzania języka naturalnego)*\n",
    "\n",
    "Spróbujemy wymodelować działanie na wektorach i sprawdzić do jakiego wektora wynik jest najbardziej podobny w sensie podobieństwa znaczenia słowa. ( podobieństwo wektorów wystarczy, sprawdziliśmy już wczesniej że to działa nawet dla najprostszej miary cosinusowej)\n",
    "\n",
    "\n",
    "Użyjemy Glove, korpus 6B.50d, z poniższego linka (pośrednio, ściągamy zipa i zostawiamy tylko jeden plik, ważne w jakim katalogu żeby dobrze się zlinkowało).\n",
    "Zostawiamy najmniejszy plik, żebyśmy zdążyli policzyć. (można znaleźć link do pojedynczego zbiory glove.6B.50d.txt, wtedy nie ma potrzeby kasowania)\n",
    "\n",
    "\n",
    "To jest 50 wymiarowe embeddings, co oznacza że każdy wektor ma 50 współrzędnych.\n",
    "\n",
    "Stanford Glove website: \n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/.\n",
    "\n",
    "\n",
    "\n",
    "glove.6B.50d.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "path = r\"models/glove.6B.50d.txt\"\n",
    "words = pd.read_table(path, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otrzymamy 50 wymiarowe wektory z 400k słów.\n",
    "\n",
    "Możemy obejrzeć jeden z nich, na przykład wektor\n",
    "\n",
    "*Paris*\n",
    "\n",
    "**Wyświetl wektor Paris** i wyjaśnij znaczenie współrzędnych tego wektora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "9",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "11",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "12",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "13",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "14",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "16",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "17",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "18",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "19",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "20",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "21",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "22",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "23",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "24",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "25",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "26",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "27",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "28",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "29",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "30",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "31",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "32",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "33",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "34",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "35",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "36",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "37",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "38",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "39",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "40",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "41",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "42",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "43",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "44",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "45",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "46",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "47",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "48",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "49",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "50",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "312e30d4-0082-4829-847b-4b57db95201e",
       "rows": [
        [
         "paris",
         "0.76989",
         "1.181",
         "-1.1299",
         "-0.74725",
         "-0.5969",
         "-1.0518",
         "-0.46552",
         "0.27009",
         "-0.99243",
         "-0.04864",
         "0.28642",
         "-0.75261",
         "-1.0566",
         "-0.19205",
         "0.572",
         "-0.24391",
         "-0.36054",
         "-0.70876",
         "-0.91951",
         "-0.27024",
         "1.5131",
         "1.0313",
         "-0.55713",
         "0.52952",
         "-0.71494",
         "-1.0949",
         "-0.60565",
         "0.31329",
         "-0.44488",
         "0.55915",
         "2.1429",
         "0.43389",
         "-0.5529",
         "-0.24261",
         "-0.43679",
         "-0.96014",
         "0.25828",
         "0.79385",
         "0.37132",
         "0.49623",
         "0.84359",
         "-0.25875",
         "1.5616",
         "-1.1199",
         "0.091676",
         "0.076675",
         "-0.45084",
         "-0.86104",
         "0.97599",
         "-0.35615"
        ]
       ],
       "shape": {
        "columns": 50,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>paris</th>\n",
       "      <td>0.76989</td>\n",
       "      <td>1.181</td>\n",
       "      <td>-1.1299</td>\n",
       "      <td>-0.74725</td>\n",
       "      <td>-0.5969</td>\n",
       "      <td>-1.0518</td>\n",
       "      <td>-0.46552</td>\n",
       "      <td>0.27009</td>\n",
       "      <td>-0.99243</td>\n",
       "      <td>-0.04864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.84359</td>\n",
       "      <td>-0.25875</td>\n",
       "      <td>1.5616</td>\n",
       "      <td>-1.1199</td>\n",
       "      <td>0.091676</td>\n",
       "      <td>0.076675</td>\n",
       "      <td>-0.45084</td>\n",
       "      <td>-0.86104</td>\n",
       "      <td>0.97599</td>\n",
       "      <td>-0.35615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1      2       3        4       5       6        7        8   \\\n",
       "0                                                                          \n",
       "paris  0.76989  1.181 -1.1299 -0.74725 -0.5969 -1.0518 -0.46552  0.27009   \n",
       "\n",
       "            9        10  ...       41       42      43      44        45  \\\n",
       "0                        ...                                               \n",
       "paris -0.99243 -0.04864  ...  0.84359 -0.25875  1.5616 -1.1199  0.091676   \n",
       "\n",
       "             46       47       48       49       50  \n",
       "0                                                    \n",
       "paris  0.076675 -0.45084 -0.86104  0.97599 -0.35615  \n",
       "\n",
       "[1 rows x 50 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kod \n",
    "words[words.index == \"paris\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz znajdźmy embedding dla słów, które się w tym zbiorze znajdują:\n",
    "\n",
    "*France, Italy, Paris, Rome*\n",
    "\n",
    "\n",
    "można użyć np. *word loc*, ale...\n",
    "\n",
    " nie trzeba; można stworzyć macierz dowolna metodą\n",
    "\n",
    "Wyświetl macierz która powstanie, oraz macierz po normalizacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#miejsce na kod macierzy coocurence, (znormalizowanej również)\n",
    "import numpy as np\n",
    "\n",
    "selected_words = [\"france\", \"italy\", \"paris\", \"rome\"]\n",
    "\n",
    "embeddings = words.loc[selected_words]\n",
    "\n",
    "coocurence_matrix = np.dot(embeddings, embeddings.T)\n",
    "coocurence_df = pd.DataFrame(coocurence_matrix, index=selected_words, columns=selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "france",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "italy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "paris",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rome",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "96ccfb03-bd1c-40c2-9de1-3af3a8f638a1",
       "rows": [
        [
         "france",
         "35.617476359883206",
         "26.39846926218464",
         "26.571788040327995",
         "17.723144661925105"
        ],
        [
         "italy",
         "26.39846926218464",
         "32.253093384613365",
         "18.623831698975998",
         "20.89440262776776"
        ],
        [
         "paris",
         "26.571788040327995",
         "18.623831698975998",
         "30.77887021710101",
         "19.29898620805099"
        ],
        [
         "rome",
         "17.723144661925105",
         "20.89440262776776",
         "19.29898620805099",
         "24.008865411037647"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>france</th>\n",
       "      <th>italy</th>\n",
       "      <th>paris</th>\n",
       "      <th>rome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>france</th>\n",
       "      <td>35.617476</td>\n",
       "      <td>26.398469</td>\n",
       "      <td>26.571788</td>\n",
       "      <td>17.723145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italy</th>\n",
       "      <td>26.398469</td>\n",
       "      <td>32.253093</td>\n",
       "      <td>18.623832</td>\n",
       "      <td>20.894403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paris</th>\n",
       "      <td>26.571788</td>\n",
       "      <td>18.623832</td>\n",
       "      <td>30.778870</td>\n",
       "      <td>19.298986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rome</th>\n",
       "      <td>17.723145</td>\n",
       "      <td>20.894403</td>\n",
       "      <td>19.298986</td>\n",
       "      <td>24.008865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           france      italy      paris       rome\n",
       "france  35.617476  26.398469  26.571788  17.723145\n",
       "italy   26.398469  32.253093  18.623832  20.894403\n",
       "paris   26.571788  18.623832  30.778870  19.298986\n",
       "rome    17.723145  20.894403  19.298986  24.008865"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coocurence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = coocurence_matrix.sum(axis=1, keepdims=True)\n",
    "normalized_matrix = coocurence_matrix / row_sums\n",
    "normalized_df = pd.DataFrame(normalized_matrix, index=selected_words, columns=selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "france",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "italy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "paris",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rome",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0c37a6c4-f857-4fb2-bc93-42773a72a6fc",
       "rows": [
        [
         "france",
         "0.3350313431822614",
         "0.24831390426153044",
         "0.24994420570269257",
         "0.16671054685351563"
        ],
        [
         "italy",
         "0.2689062224433389",
         "0.3285439552585207",
         "0.18971040251815333",
         "0.21283941977998708"
        ],
        [
         "paris",
         "0.2789001630890553",
         "0.1954776129594404",
         "0.32305812127577005",
         "0.20256410267573435"
        ],
        [
         "rome",
         "0.2163327234043578",
         "0.2550418173859891",
         "0.23556780272182906",
         "0.29305765648782406"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>france</th>\n",
       "      <th>italy</th>\n",
       "      <th>paris</th>\n",
       "      <th>rome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>france</th>\n",
       "      <td>0.335031</td>\n",
       "      <td>0.248314</td>\n",
       "      <td>0.249944</td>\n",
       "      <td>0.166711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italy</th>\n",
       "      <td>0.268906</td>\n",
       "      <td>0.328544</td>\n",
       "      <td>0.189710</td>\n",
       "      <td>0.212839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paris</th>\n",
       "      <td>0.278900</td>\n",
       "      <td>0.195478</td>\n",
       "      <td>0.323058</td>\n",
       "      <td>0.202564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rome</th>\n",
       "      <td>0.216333</td>\n",
       "      <td>0.255042</td>\n",
       "      <td>0.235568</td>\n",
       "      <td>0.293058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          france     italy     paris      rome\n",
       "france  0.335031  0.248314  0.249944  0.166711\n",
       "italy   0.268906  0.328544  0.189710  0.212839\n",
       "paris   0.278900  0.195478  0.323058  0.202564\n",
       "rome    0.216333  0.255042  0.235568  0.293058"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No i sprawdżmy czy matematyka na słowach zadziała\n",
    "\n",
    "czyli\n",
    "\n",
    "*Paryż-Francja+Włochy=Rzym*\n",
    "\n",
    "Stwórz wektor Paris-France+Italy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# miejsce na kod\n",
    "left = words.loc[\"paris\"].values - words.loc[\"france\"] + words.loc[\"italy\"]\n",
    "right = words.loc[\"rome\"]\n",
    "(left == right).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zbadaj podobieństwo pomiędzy stworzonym wektorem a innymi słowami wyświetlając 10 najbardziej podobnych do niego słów \n",
    "\n",
    "( np.  używając find_closest_word, ale można też uzyć innej metody liczenia podobieństwa, szukaj metod similarity, \n",
    "\n",
    "wybierając metode pamiętaj, że poruszamy się wyważając korzyści między jakością wyniku a czasem obliczeń)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.87599809,  9.74306152, 10.58793184, ..., -8.42813627,\n",
       "       -3.25354317, -3.25354317])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(words, right.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.dot(words, right.T)\n",
    "b = pd.DataFrame(a, index=words.index, columns=[\"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "distance",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f63da03a-62eb-4d94-8ea0-3664dbbb0a33",
       "rows": [
        [
         "rome",
         "24.008865411037647"
        ],
        [
         "italy",
         "20.89440262776776"
        ],
        [
         "cathedral",
         "20.577377084179993"
        ],
        [
         "milan",
         "20.175302951533002"
        ],
        [
         "sicherle",
         "20.092325977033994"
        ],
        [
         "pope",
         "20.001254254248998"
        ],
        [
         "madrid",
         "19.379516042379997"
        ],
        [
         "paris",
         "19.29898620805099"
        ],
        [
         "roman",
         "19.273272572593996"
        ],
        [
         "palace",
         "18.8550908088694"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rome</th>\n",
       "      <td>24.008865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italy</th>\n",
       "      <td>20.894403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cathedral</th>\n",
       "      <td>20.577377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milan</th>\n",
       "      <td>20.175303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sicherle</th>\n",
       "      <td>20.092326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pope</th>\n",
       "      <td>20.001254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>madrid</th>\n",
       "      <td>19.379516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paris</th>\n",
       "      <td>19.298986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roman</th>\n",
       "      <td>19.273273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>palace</th>\n",
       "      <td>18.855091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            distance\n",
       "0                   \n",
       "rome       24.008865\n",
       "italy      20.894403\n",
       "cathedral  20.577377\n",
       "milan      20.175303\n",
       "sicherle   20.092326\n",
       "pope       20.001254\n",
       "madrid     19.379516\n",
       "paris      19.298986\n",
       "roman      19.273273\n",
       "palace     18.855091"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kod \n",
    "def find_closest_words(words, embedding, top_n=10):\n",
    "    distances = np.dot(words, embedding.T)\n",
    "    coocurence_df = pd.DataFrame(distances, index=words.index, columns=[\"distance\"]).sort_values(by=[\"distance\"], ascending=False)\n",
    "    return coocurence_df.iloc[0:top_n, :]\n",
    "\n",
    "find_closest_words(words, right, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czy najbardziej podobny do tego  wektora Paris-France+Italy jest wektor Rome ?\n",
    "\n",
    "Czy to oznacza że taką relacje można powtórzyć dla innych zestawów słów?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color: yellow;\">Zadanie 2 </p> 🐳\n",
    "\n",
    "\n",
    " Powtórz działania dla poniższych zestawów :\n",
    "\n",
    "**water-wet+fire= flames**\n",
    "\n",
    "\n",
    "**winter-cold+summer=warm**\n",
    "\n",
    "\n",
    "oraz klasycznego:\n",
    "\n",
    "**king-men+woman=queen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kod zadania 2\n",
    "def compare_vectors(index_1, index_2, index_3, index_4, words, top_n=10):\n",
    "    left = words.loc[index_1] - words.loc[index_2] + words.loc[index_3]\n",
    "    right = words.loc[index_4]\n",
    "    \n",
    "    print(f\"Comparison: {(left==right).sum()}\")\n",
    "    \n",
    "    coocurence_df = find_closest_words(words, right, 10)\n",
    "    print(coocurence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: 0\n",
      "            distance\n",
      "0                   \n",
      "flames     28.240650\n",
      "nahn       22.579730\n",
      "debris     20.848968\n",
      "billowed   20.623328\n",
      "smoke      20.148238\n",
      "tear       19.056865\n",
      "billowing  18.841191\n",
      "blaze      18.731457\n",
      "avalanche  18.456331\n",
      "debehogne  18.378496\n"
     ]
    }
   ],
   "source": [
    "compare_vectors(\"water\", \"wet\", \"fire\", \"flames\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: 0\n",
      "                 distance\n",
      "0                        \n",
      "warm            29.908686\n",
      "1-800-535-4425  28.739116\n",
      "moist           24.262903\n",
      "dry             23.730344\n",
      "temperatures    22.507354\n",
      "drizzle         22.437515\n",
      "warmer          22.102078\n",
      "humid           22.089390\n",
      "rain            21.985990\n",
      "cold            21.807384\n"
     ]
    }
   ],
   "source": [
    "compare_vectors(\"winter\", \"cold\", \"summer\", \"warm\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: 0\n",
      "           distance\n",
      "0                  \n",
      "queen     26.964749\n",
      "princess  25.256698\n",
      "king      21.877506\n",
      "throne    21.784407\n",
      "royal     21.776677\n",
      "daughter  21.687536\n",
      "her       21.545704\n",
      "empress   20.825105\n",
      "prince    20.645073\n",
      "mother    20.501926\n"
     ]
    }
   ],
   "source": [
    "compare_vectors(\"king\", \"men\", \"women\", \"queen\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oprócz GloVe, istnieje kilka innych pretrenowanych embeddingów, które mogą być użyte do klasycznego zadania analogii, takiego jak \n",
    "\n",
    "\"król - mężczyzna + kobieta = królowa\" (Word2Vec i FasTex powinny działać również po polsku). \n",
    "\n",
    "## <p style=\"color: #6495ED;\">Zadanie 3 </p> 🌍\n",
    "Pobierz inne pretrenowane modele (np. Word2Vec, FastText, BERT)\n",
    "<p style=\"color: red;\">[dla chetnych] </p>  ELMo (Embeddings from Language Models) \n",
    "\n",
    "i dla każdego z nich powtórz zestaw angielskich analogii z poprzednich zadań.\n",
    "\n",
    "Następnie spróbuj dodać polskie:\n",
    "\n",
    "* **Warszawa - Polska + Niemcy = Berlin** \n",
    "* **Król - mężczyzna + kobieta = królowa** \n",
    "* **Paryż - Francja + Włochy = Rzym**\n",
    "* **Siostra - kobieta + mężczyzna = brat** \n",
    "\n",
    "Porównaj jakość działania róznych modeli, również dla polskich zestawów. \n",
    "Możesz spróbować poprawić wyniki znajdując polskie modele.\n",
    "\n",
    "Biblioteki:\n",
    "\n",
    "* pip install gensim (word2Vec, FastTex)\n",
    "* pip install transformers torch (BERT)\n",
    "* <p style=\"color: red;\">[dla chetnych] </p>  pip install allennlp (ElMo)\n",
    "\n",
    "\n",
    "Modele:\n",
    "* word2vec-google-news-300\n",
    "* fasttext-wiki-news-subwords-300\n",
    "* bert-base-uncased\n",
    "* <p style=\"color: red;\">[dla chetnych] </p>   Konfiguracja ELMo:\n",
    "\n",
    "    options_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096x512_2048cnn_2xhighway/elmo_2x4096x512_2048cnn_2xhighway_options.json\"\n",
    "\n",
    "    weight_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096x512_2048cnn_2xhighway/elmo_2x4096x512_2048cnn_2xhighway_weights.hdf5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kod zadania 3\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "fasttext_model = KeyedVectors.load(r'models/fasttext-wiki-news-subwords-300.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity_f(model, words):\n",
    "    left = model[words[0]] - model[words[1]] + model[words[2]]\n",
    "    right = model[words[3]]\n",
    "\n",
    "    print(f\"Comparison: {(left == right).sum()}\")\n",
    "    print(f\"Similar: {model.most_similar(left)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: 0\n",
      "Similar: [('king', 0.8453638553619385), ('queen', 0.71834397315979), ('king-', 0.678547203540802), ('monarch', 0.6708823442459106), ('child-king', 0.6690906286239624), ('boy-king', 0.6576582789421082), ('king-making', 0.6454074382781982), ('kingship', 0.6315165758132935), ('princess', 0.6250572800636292), ('prince', 0.6195020079612732)]\n",
      "Comparison: 0\n",
      "Similar: [('Italy', 0.8525390028953552), ('Rome', 0.8008622527122498), ('Milan', 0.792957067489624), ('Bologna', 0.784528374671936), ('Naples', 0.7842305302619934), ('Italy-', 0.7604862451553345), ('Paris', 0.7551395297050476), ('Venice', 0.7510690093040466), ('Milano', 0.7439693808555603), ('Parioli', 0.7396515011787415)]\n",
      "Comparison: 0\n",
      "Similar: [('Berlin', 0.7403421401977539), ('Germany', 0.7287525534629822), ('Warsaw', 0.7193452715873718), ('Munich', 0.7039551734924316), ('Hamburg', 0.6924000978469849), ('Stuttgart', 0.6905930042266846), ('Frankfurt', 0.6903912425041199), ('Bonn', 0.6753316521644592), ('Cologne', 0.6714510321617126), ('Dresden', 0.6614290475845337)]\n",
      "Comparison: 0\n",
      "Similar: [('sister', 0.846070408821106), ('brother', 0.8282320499420166), ('brothers', 0.727699339389801), ('sisters', 0.7184571027755737), ('brother-in-law', 0.7170220017433167), ('cousin', 0.7140408158302307), ('sister-in-law', 0.6763167381286621), ('stepbrother', 0.6760019063949585), ('uncle', 0.6749981641769409), ('cousins', 0.6727116703987122)]\n"
     ]
    }
   ],
   "source": [
    "check_similarity_f(fasttext_model, [\"king\", \"men\", \"women\", \"queen\"])\n",
    "check_similarity_f(fasttext_model, [\"Paris\", \"France\", \"Italy\", \"Rome\"])\n",
    "check_similarity_f(fasttext_model, [\"Warsaw\", \"Poland\", \"Germany\", \"Berlin\"])\n",
    "check_similarity_f(fasttext_model, [\"sister\", \"women\", \"men\", \"brother\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(r\"models/bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(r\"models/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity_b(model, tokenizer, words):\n",
    "    word_embeddings_matrix = model.embeddings.word_embeddings.weight\n",
    "    \n",
    "    ids_words = tokenizer(words, add_special_tokens=False)['input_ids']\n",
    "    ids_words = [id_list[0] for id_list in ids_words]\n",
    "    \n",
    "    embedding_1 = word_embeddings_matrix[ids_words[0]] \n",
    "    embedding_2 = word_embeddings_matrix[ids_words[1]] \n",
    "    embedding_3 = word_embeddings_matrix[ids_words[2]] \n",
    "    embedding_4 = word_embeddings_matrix[ids_words[3]] \n",
    "    \n",
    "    result_vector = embedding_1 - embedding_2 + embedding_3\n",
    "    print(f\"Comparison: {(result_vector == embedding_4).sum()}\")\n",
    "    \n",
    "    result_vector_normalized = F.normalize(result_vector.unsqueeze(0), p=2, dim=1)\n",
    "    word_embeddings_matrix_normalized = F.normalize(word_embeddings_matrix, p=2, dim=1)\n",
    "    \n",
    "    similarities = torch.matmul(result_vector_normalized, word_embeddings_matrix_normalized.transpose(0, 1))\n",
    "    \n",
    "    similarities = similarities.squeeze(0)\n",
    "    \n",
    "    sorted_indices = torch.argsort(similarities, descending=True)\n",
    "    top_k = 5\n",
    "    top_k_indices = sorted_indices[ : 3 + top_k]\n",
    "    top_k_similarities = similarities[top_k_indices]\n",
    "    top_k_words = tokenizer.convert_ids_to_tokens(sorted_indices)\n",
    "\n",
    "    \n",
    "    print(f\"Similar:\")\n",
    "    for word, sim in zip(top_k_words, top_k_similarities):\n",
    "        print(f\"  {word}: {sim.item():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: 0\n",
      "Similar:\n",
      "  king: 0.7536\n",
      "  queen: 0.6097\n",
      "  women: 0.4743\n",
      "  kings: 0.4737\n",
      "  queens: 0.4304\n",
      "  prince: 0.4302\n",
      "  princess: 0.4298\n",
      "  ruler: 0.4134\n",
      "Comparison: 0\n",
      "Similar:\n",
      "  warsaw: 0.7318\n",
      "  germany: 0.6584\n",
      "  berlin: 0.5645\n",
      "  munich: 0.5633\n",
      "  germans: 0.5404\n",
      "  darmstadt: 0.5385\n",
      "  vienna: 0.5370\n",
      "  stuttgart: 0.5361\n",
      "Comparison: 0\n",
      "Similar:\n",
      "  paris: 0.7137\n",
      "  italy: 0.6913\n",
      "  italian: 0.5560\n",
      "  milan: 0.5440\n",
      "  parisian: 0.5329\n",
      "  italians: 0.5269\n",
      "  naples: 0.5162\n",
      "  rome: 0.5144\n",
      "Comparison: 0\n",
      "Similar:\n",
      "  sister: 0.7488\n",
      "  brother: 0.6498\n",
      "  daughter: 0.4801\n",
      "  men: 0.4801\n",
      "  son: 0.4672\n",
      "  father: 0.4654\n",
      "  sisters: 0.4611\n",
      "  niece: 0.4503\n"
     ]
    }
   ],
   "source": [
    "check_similarity_b(bert_model, bert_tokenizer, [\"king\", \"men\", \"women\", \"queen\"])\n",
    "check_similarity_b(bert_model, bert_tokenizer, [\"Warsaw\", \"Poland\", \"Germany\", \"Berlin\"])\n",
    "check_similarity_b(bert_model, bert_tokenizer, [\"Paris\", \"France\", \"Italy\", \"Rome\"])\n",
    "check_similarity_b(bert_model, bert_tokenizer, [\"sister\", \"women\", \"men\", \"brother\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_model = KeyedVectors.load(r\"models/word2vec-google-news-300/word2vec-google-news-300.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(model, words):\n",
    "    left = model[words[0]] - model[words[1]] + model[words[2]]\n",
    "    right = model[words[3]]\n",
    "\n",
    "    print(f\"Comparison: {(left == right).sum()}\")\n",
    "    print(f\"Similar: {model.most_similar(left)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: 1\n",
      "Similar: [('king', 0.8527070879936218), ('queen', 0.6743921637535095), ('monarch', 0.6191632151603699), ('kings', 0.5753854513168335), ('crown_prince', 0.562209963798523), ('princess', 0.543317437171936), ('prince', 0.5246986150741577), ('sultan', 0.5236638784408569), ('ruler', 0.5165805220603943), ('monarchy', 0.5113592147827148)]\n",
      "Comparison: 1\n",
      "Similar: [('Milan', 0.7251641750335693), ('Rome', 0.7073156237602234), ('Italy', 0.684763491153717), ('Paris', 0.6693719625473022), ('Palermo_Sicily', 0.6014109253883362), ('Italian', 0.596384584903717), ('Tuscany', 0.5672276020050049), ('Sicily', 0.5654013752937317), ('Bologna', 0.5645887851715088), ('Bologna_Italy', 0.5513802170753479)]\n",
      "Comparison: 1\n",
      "Similar: [('Berlin', 0.7192326188087463), ('Warsaw', 0.7112792134284973), ('Frankfurt', 0.6706617474555969), ('Dusseldorf', 0.6601549386978149), ('Stuttgart', 0.6353572607040405), ('Munich', 0.6288971900939941), ('Germany', 0.6251928806304932), ('Vienna', 0.6169423460960388), ('Hamburg', 0.6073066592216492), ('Cologne', 0.6056288480758667)]\n",
      "Comparison: 0\n",
      "Similar: [('sister', 0.8361483812332153), ('brother', 0.7752998471260071), ('cousin', 0.7571418285369873), ('uncle', 0.666831374168396), ('younger_brother', 0.6621278524398804), ('nephew', 0.6567128300666809), ('son', 0.6531183123588562), ('aunt', 0.6464547514915466), ('father', 0.6464194655418396), ('siblings', 0.6383066177368164)]\n"
     ]
    }
   ],
   "source": [
    "check_similarity(word2vec_model, [\"king\", \"men\", \"women\", \"queen\"])\n",
    "check_similarity(word2vec_model, [\"Paris\", \"France\", \"Italy\", \"Rome\"])\n",
    "check_similarity(word2vec_model, [\"Warsaw\", \"Poland\", \"Germany\", \"Berlin\"])\n",
    "check_similarity(word2vec_model, [\"sister\", \"women\", \"men\", \"brother\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Które embeddingi lepiej odwzorowują zależności między słowami ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color: magenta;\">Zadanie 4 </p> 🐊\n",
    "\n",
    "Użyj pretrenowanych embeddingów do pokazania wieloznaczności słowa \"bank\". \n",
    "\n",
    "Zadanie polega na znalezieniu najbliższych angielskich słów do podanego niejednoznaczmego słowa i ustalenia kontekstu \n",
    "*<p style=\"color:yellow;\">\"bank\"</p>* \n",
    "\n",
    "(\"bank\" jako instytucja finansowa i \"bank\" jako brzeg rzeki)\n",
    "\n",
    "Użyj modelu Word2Vec. \n",
    "\n",
    "Znajdź najbliższe (najbardziej podobne ) słowa ( 10 z najwyzszym podobieństwem) i na tej podstawie ustal kontekst w jakim słowo zostało zrozumiane.\n",
    "Mozna spróbowac wymusić konkretny kontekst szukając podobieństw do \"river_bank\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('banks', 0.7440759539604187),\n",
       " ('banking', 0.6901614665985107),\n",
       " ('Bank', 0.6698698401451111),\n",
       " ('lender', 0.634228527545929),\n",
       " ('banker', 0.6092953681945801),\n",
       " ('depositors', 0.6031531691551208),\n",
       " ('mortgage_lender', 0.5797975659370422),\n",
       " ('depositor', 0.5716428160667419),\n",
       " ('BofA', 0.5714625120162964),\n",
       " ('Citibank', 0.5589520931243896)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kod zadania 4\n",
    "\n",
    "word2vec_model.most_similar(\"bank\"[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color: pink;\">Zadanie 5 </p> 🦀\n",
    "\n",
    "Powtórz zadanie 4 dla : \n",
    " \n",
    "  - *bat* (nietoperz vs kij do baseballa) \n",
    "  - *apple* (owoc vs firma technologiczna) \n",
    "  - *python* (język programowania vs wąż)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apples', 0.720359742641449),\n",
       " ('pear', 0.6450697183609009),\n",
       " ('fruit', 0.6410146355628967),\n",
       " ('berry', 0.6302294731140137),\n",
       " ('pears', 0.6133960485458374),\n",
       " ('strawberry', 0.6058261394500732),\n",
       " ('peach', 0.6025872826576233),\n",
       " ('potato', 0.5960935354232788),\n",
       " ('grape', 0.5935865044593811),\n",
       " ('blueberry', 0.5866668224334717)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kod zadania 5\n",
    "word2vec_model.most_similar(\"apple\"[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bats', 0.7677518725395203),\n",
       " ('batting', 0.6346984505653381),\n",
       " ('Pinch_hitter_Brayan_Pena', 0.6011941432952881),\n",
       " ('batsman', 0.5579798817634583),\n",
       " ('batted', 0.5542199611663818),\n",
       " ('Hawaiian_hoary', 0.5447419881820679),\n",
       " ('Lelands.com_auctioned', 0.5397745370864868),\n",
       " ('yelled_Cheater', 0.5380043983459473),\n",
       " ('wicketkeeper_Andrew_Hodd', 0.5371009111404419),\n",
       " ('lefthanded_batter', 0.53566575050354)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(\"bat\"[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pythons', 0.6688377857208252),\n",
       " ('Burmese_python', 0.6680365204811096),\n",
       " ('snake', 0.6606293320655823),\n",
       " ('crocodile', 0.6591362953186035),\n",
       " ('boa_constrictor', 0.6443519592285156),\n",
       " ('alligator', 0.6421657204627991),\n",
       " ('reptile', 0.6387745141983032),\n",
       " ('albino_python', 0.6158880591392517),\n",
       " ('croc', 0.6083583831787109),\n",
       " ('lizard', 0.6013416647911072)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(\"python\"[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color: #7FFFD4\">Zadanie 6 </p> 🏟\n",
    "Powtórz instrukcję zadania 4 dla polskich słów o różnych znaczeniach: \n",
    "  - *zamek* (budowla vs mechanizm zamykający), \n",
    "  - *piła* (narzędzie vs czasownik w przeszłości)\n",
    "  - *list* (zapis wiadomości vs nachylenie statku; w kontekście żeglugi, \"list\" oznacza kąt nachylenia statku na boki (do portu lub do sterburty) w stanie równowagi, bez działania zewnętrznych sił\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Zamek', 0.8126124739646912),\n",
       " ('zámek', 0.8014318943023682),\n",
       " ('Ogród', 0.7847594618797302),\n",
       " ('zamku', 0.7834646701812744),\n",
       " ('rynek', 0.7772862911224365),\n",
       " ('rzecz', 0.7753502130508423),\n",
       " ('Dziwnów', 0.7723685503005981),\n",
       " ('rzeczy', 0.7715327143669128),\n",
       " ('Książ', 0.7680349946022034),\n",
       " ('Podzamcze', 0.7647734880447388)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kod zadania 6\n",
    "fasttext_model.most_similar(\"zamek\"[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pilau', 0.7341576218605042),\n",
       " ('pilav', 0.7166672348976135),\n",
       " ('pilum', 0.6975015997886658),\n",
       " ('palla', 0.6828243136405945),\n",
       " ('Mpila', 0.6811730861663818),\n",
       " ('pilaris', 0.6811076402664185),\n",
       " ('maro', 0.6795235872268677),\n",
       " ('bura', 0.6751182079315186),\n",
       " ('pilate', 0.6681479811668396),\n",
       " ('pilar', 0.6631638407707214)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.most_similar(\"pila\"[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lists', 0.84706711769104),\n",
       " ('listing', 0.7999898195266724),\n",
       " ('sub-list', 0.7774856686592102),\n",
       " ('listed', 0.7744861841201782),\n",
       " ('list-', 0.7545185089111328),\n",
       " ('lists-', 0.7412810325622559),\n",
       " ('non-list', 0.739302933216095),\n",
       " ('list--', 0.7241465449333191),\n",
       " ('list-table', 0.7234554886817932),\n",
       " ('listable', 0.7148085832595825)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.most_similar(\"list\"[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 😎 Wnioski\n",
    "* Który model działał najlepiej?\n",
    "\n",
    "\n",
    "* Który model działał najszybciej?\n",
    "\n",
    "* Który był najłatwiejszy w uzyciu?\n",
    "\n",
    "\n",
    "* Który zadziałał dla polskiego?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
